{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martine1grace/NLP_FELLOWSHIP_AI-Machine_Translation.-/blob/main/Week4/EasyNMT_Colab_REST_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkMrIT2H97tF"
      },
      "source": [
        "# EasyNMT REST API using Colab\n",
        "\n",
        "This notebook demonstrates, how we can create a REST API on Google Colab that runs [EasyNMT](https://github.com/UKPLab/EasyNMT).\n",
        "\n",
        "You can then query this API from your local machine using standard GET / POST requests and translate documents to 150+ languages.\n",
        "\n",
        "This allows you to use the free GPU from Colab for Neural Machine Translation with the simplicity of quering a REST API for machine translation.\n",
        "\n",
        "**Note**: **Colab has a runtime restrictions**, that a notebook can only run up to a certain time limit. It is then stopped after this time limit. This notebook will **not work** if you want to host a translation API permanently. But you can use this notebook to start a REST API, and then calling it from your machine to translate larger amount of text. [more info](https://research.google.com/colaboratory/faq.html#resource-limits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLavrObz_IPM"
      },
      "source": [
        "## Colab with GPU\n",
        "When running this notebook in colab, ensure that you run it with a GPU as hardware accelerator. To enable this:\n",
        "- Navigate to Edit → Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "With `!nvidia-smi` we can check which GPU was assigned to us in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z36REF9Q_IZH",
        "outputId": "263d68a3-d744-49bd-950c-ed382bdd46a2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar 16 21:41:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf2GQJKY_D6R"
      },
      "source": [
        "## EasyNMT Installation\n",
        "First we install EasyNMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7QYzqUZ8TNK"
      },
      "source": [
        "!pip install easynmt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT-1WDq2AGxU"
      },
      "source": [
        "## Other dependencies\n",
        "We create our REST API using [FastAPI](https://fastapi.tiangolo.com/). In order to host this in a Colab, we use [pyngrok](https://github.com/alexdlaird/pyngrok).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5lhTs9H7lPt"
      },
      "source": [
        "!pip install fastapi pyngrok  uvicorn nest-asyncio "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMJyKREr_-7v"
      },
      "source": [
        "## Translation Model\n",
        "Then we define our translation model. See the [EasyNMT documentation](https://github.com/UKPLab/EasyNMT) for more details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6tSQL658kVz"
      },
      "source": [
        "from easynmt import EasyNMT\n",
        "model = EasyNMT('opus-mt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWoC1KXnAOL-"
      },
      "source": [
        "# REST API\n",
        "Next we define a simple REST API.\n",
        "\n",
        "We define a GET and a POST method, so that you can later query the REST API with GET and POST requests.\n",
        "\n",
        "The API accepts three parameters:\n",
        "- target_lang: Our target language for the translation\n",
        "- text: A list of texts we want to translate\n",
        "- source_lang: Optional. The source language for all texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q8IsN0L7pIX"
      },
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from typing import Optional, Union, List\n",
        "from fastapi import FastAPI, HTTPException, Query, Request\n",
        "\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def translate_get(target_lang: str, text: List[str] = Query([]), source_lang: Optional[str] = None):\n",
        "  return model.translate(text, target_lang=target_lang, source_lang=source_lang)\n",
        "\n",
        "@app.post(\"/\")\n",
        "async def translate_post(request: Request):\n",
        "    data = await request.json()\n",
        "    return translate_get(**data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3d4nI8BQlu"
      },
      "source": [
        "# Host REST-API on Colab\n",
        "Finally we start our REST API and host it on Colab.\n",
        "\n",
        "As output, the public URL will be displayed which you can use to translate documents.\n",
        "\n",
        "Either by using a GET request (e.g. opening the following URL in your browser):\n",
        "`http://[auto_id_from_ngrok].ngrok.io?target_lang=en&text=[Your_Text]`\n",
        "\n",
        "Or via a POST request:\n",
        "```\n",
        "import requests\n",
        "r = requests.post(\"http://[auto_id_from_ngrok].ngrok.io\", \n",
        "     json={'target_lang': 'en', 'text': [\"Hallo Welt\", \"Es werden alle Texte übersetzt\"]})\n",
        "print(r.json())\n",
        "```\n",
        "\n",
        "Execute the following cell and let it run as long as you like to have the translation REST API hosted. Stop the next cell to shutdown the REST API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgFFt8lp7sQ8",
        "outputId": "06582c1f-3a40-48e8-8ac2-458de49706f9"
      },
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"REST API started\")\n",
        "print(\"Your public API URL:\", ngrok_tunnel.public_url)\n",
        "print(\"You can for example open the following URL in your browser: {}?target_lang=en&text=Hallo%20Welt\".format(ngrok_tunnel.public_url))\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REST API started\n",
            "Your public API URL: http://f96dc72838ff.ngrok.io\n",
            "You can for example open the following URL in your browser: http://f96dc72838ff.ngrok.io?target_lang=en&text=Hallo%20Welt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [58]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     2003:d4:bf3f:113a:2455:cce5:914b:2876:0 - \"GET /?target_lang=en&text=Hallo%20Welt HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3221: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     2003:d4:bf3f:113a:2455:cce5:914b:2876:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     18.190.26.136:0 - \"GET / HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     18.190.26.136:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     18.221.194.75:0 - \"GET / HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     18.221.194.75:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}